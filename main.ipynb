{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6X6UIG6qwm4L"
   },
   "source": [
    "<h1 align=\"center\"><font face = \"Times New Roman\" size = \"80\">JPEG Compression</font></h1>\n",
    "<h2 align=\"center\"><font face = \"Times New Roman\" size = \"\"> Information theory and coding</font></h2>\n",
    "<h3 align=\"center\"><font face = \"Times New Roman\"> Fall 2019</font></h3>\n",
    "<h5 align=\"center\"><font face = \"Times New Roman\"\">Alaa Mohamed Roshdy&nbsp 201600031</font></h5>\n",
    "<h5 align=\"center\"><font face = \"Times New Roman\">Mohamed Adham Mahrous&nbsp 201601078</font></h5>\n",
    "<h5 align=\"center\"><font face = \"Times New Roman\">Mohamed Mostafa Hamed&nbsp 201600236</font></h5>\n",
    "<h5 align=\"center\"><font face = \"Times New Roman\">Mohamed Kasem Saber&nbsp 201601144</font></h5>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMKQ5L8Ubv1B"
   },
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "JPEG compression is one of the most frequently used form of lossy compression for images. In this paper, we implement JPEG and JPEG_2000 compression and compare different compression types together, computing their compression ratios, root mean square error, and number of FLOP operations for each compression.\n",
    "For JPEG, we use 2 different block sizes for compression. Namely, 8x8 and 16x16. Furthermore, different quantization tables were used. Low quantization table ranges its values from 1-16, and high quantization table ranges its values from 1-256. \n",
    "For JPEG_2000, 2 different quantization arrays are used to quantize the DWT decomposed images. Also, DWT decomposition can be performed at an arbitrary depth of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ARDASU~1\\AppData\\Local\\Temp/ipykernel_21252/468233779.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mencoder\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreshape_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mencoder_2000\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_image\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdwt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmain\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'encoder'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "from PIL import Image\n",
    "from encoder import reshape_image\n",
    "from encoder_2000 import check_image,dwt\n",
    "import main as m\n",
    "import main_2000 as m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(im, reconstructed_image):\n",
    "    \"\"\"\n",
    "    Gets the root mean squared error between 2 images\n",
    "    Args:\n",
    "         im (numpy array) : The original image\n",
    "         reconstructed_image (numpy ndarray): The reconstructed image\n",
    "    Returns:\n",
    "        rmse: root mean squared error as a metric to compare between the original image and the reconstructed\n",
    "    \"\"\"\n",
    "    error = im - reconstructed_image\n",
    "    mse = np.sum(np.square(error)) / (im.shape[0] * im.shape[1])\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dSyFG6eXNlUC"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<h1 align=\"center\"><font face = \"Times New Roman\">JPEG compression</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4kkgGf87jF25"
   },
   "source": [
    "## Methodology\n",
    "\n",
    "In the following 4 blocks of code, we repeatedly encode and decode the image according to the given block size and quantization table. The huffman encoded sequence is saved in *encoded* array to be later used in computing the compression ratio. All reconstructed images are saved in reconstructed array, and the execution time for each compression type is saved in *execution_time* array.\n",
    "\n",
    "To encode and decode, 2 main functions in main.py were utilized; namely, encode() and decode(). Both functions arrange all encoder.py and decoder.py functions sequentially in the following way: \n",
    "\n",
    "![jpeg pipeline diagram](images/JPEG_diagram.png)\n",
    "\n",
    "### Encoding\n",
    "\n",
    "A PIL image is passed to a function where it is converted to grayscale and resized to the nearest dimensions divisible by 8. This is important since this image gets then divided into N subimages of sizes 8x8 or 16x16 depending on the input block size, and fractional numbers are not allowed for images. This resized image gets converted to a numpy array for easier computations. Each pixel is a value in the range 0-255. Then, DCT is applied on each of the N number of M x M arrays. Shown below is a gif of the DCT operation on a single 8x8 image block. Since this is 8x8 compression, there are 64 8x8 basis functions, each capturing a different frequency. The first basis function block is completely white which corresponds to the lowest frequency, and the last basis block has many small black and white boxes which correspond to the high frequency. Any image block is a combination of these basis functions. (This GIF is a snippet from Numberphile's video on JPEG compression. Link: https://www.youtube.com/watch?v=Q2aEzeMDHMA )  ![jpeg pipeline diagram](images/DCT.gif) </br>For the DCT operation, a helper function was generated to further speed up the process of compression. Instead of computing the basis functions (shown in the gif above as a checkerboard) for the DCT every single iteration on the image, the basis functions are computed only once and then used on all iterations thereafter on the image. This significantly lowered the computational time needed to compress the image. After that, one of 2 quantization tables gets used to quantize the DCT values. Quantization reduces the large DCT values that correspond to high frequencies in the image, which makes these values later encoded in lower number of bits. High compression is achieved by having large quantization values in the array. The 2D quantized array is then serialized and read in zigzag format to be represented in 1D array. Shown below is how the 2D array is read. (This GIF is a snippet from Numberphile's video on JPEG compression. Link: https://www.youtube.com/watch?v=Q2aEzeMDHMA )  ![jpeg pipeline diagram](images/serialization.gif) </br> Once done, we can see from the gif how there is a large number of repeated 0s in the whole array. These zeros can be reduced to a few bits using run-length encoding. Run-length encoding is performed on this 1D array to eliminate all the 0s, and then huffman encoding to further compress the data. The final result is a stream of bits represented in the form of string for convenience, and a dictionary for the huffman decoder to decode with.\n",
    "\n",
    "### Decoding\n",
    "\n",
    "The inverse of the process above is implemented. Using the given stream of bits and code dictionary, the data is decoded using huffman decoder, and then passed to the run-length decoder to decode the output. Then, the data is deserialized and returned to 2D arrays to be then dequantized. Dequantization is just multiplying each m x m block by the quantization array that was used in the quantization process. IDCT is then applied on each of the m x m image block and rearranged together to return a reconstructed image.\n",
    "\n",
    "\n",
    "### Analysis\n",
    "\n",
    "To measure the compression ratio for each image, the size of the image is computed in bits by multiplying the size of a single element in the array by the number of elements in the whole image. As for the encoded image, its size was computed by finding the length of the huffcoded string, since in reality, this string is a stream of bits, not a string. The size of the image is then divided by the stream of bits to compute the compression ratio. Moreover, the number of FLOPs was computed by calculating the time for the whole compression operation divided by the time for a single flop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image\n",
    "\n",
    "im = Image.open('images/joker.jpg')\n",
    "im = im.convert(\"L\")\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define block size\n",
    "box_size = 8\n",
    "encoded = [] # An array containing all 4 huffcodes resulting from the encoding process\n",
    "reconstructed = [] # Array containing all 4 reconstructed images\n",
    "execution_time = [] # Execution time for each compression\n",
    "\n",
    "start = timeit.default_timer()\n",
    "# Apply 8 x 8 compression using low compression quantization table\n",
    "\n",
    "huffcoded, code_dict, n_rows, n_columns = m.encode(im, box_size, m.table_8_low)\n",
    "encoded.append(huffcoded)\n",
    "reconstructed.append( m.decode(huffcoded, code_dict,  n_rows, n_columns, box_size, m.table_8_low))\n",
    "execution_time.append(timeit.default_timer() - start)\n",
    "\n",
    "print(\"Execution time: \", execution_time[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_size = 8\n",
    "\n",
    "start = timeit.default_timer()\n",
    "# Apply 8 x 8 compression using high compression quantization table\n",
    "\n",
    "huffcoded, code_dict, n_rows, n_columns = m.encode(im, box_size, m.table_8_high)\n",
    "encoded.append(huffcoded)\n",
    "reconstructed.append( m.decode(huffcoded, code_dict,  n_rows, n_columns, box_size, m.table_8_high) )\n",
    "execution_time.append(timeit.default_timer() - start)\n",
    "\n",
    "print(\"Execution time: \", execution_time[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_size = 16\n",
    "\n",
    "start = timeit.default_timer()\n",
    "# Apply 16 x 16 compression using low compression quantization table\n",
    "\n",
    "huffcoded, code_dict, n_rows, n_columns = m.encode(im, box_size, m.table_16_low)\n",
    "encoded.append(huffcoded)\n",
    "reconstructed.append( m.decode(huffcoded, code_dict,  n_rows, n_columns, box_size, m.table_16_low))\n",
    "execution_time.append(timeit.default_timer() - start)\n",
    "\n",
    "print(\"Execution time: \", execution_time[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_size = 16\n",
    "\n",
    "start = timeit.default_timer()\n",
    "# Apply 16 x 16 compression using high compression quantization table\n",
    "\n",
    "huffcoded, code_dict, n_rows, n_columns = m.encode(im, box_size, m.table_16_high)\n",
    "encoded.append(huffcoded)\n",
    "reconstructed.append( m.decode(huffcoded, code_dict,  n_rows, n_columns, box_size, m.table_16_high))\n",
    "execution_time.append(timeit.default_timer() - start)\n",
    "\n",
    "print(\"Execution time: \", execution_time[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zd3n6iEKREsP"
   },
   "source": [
    "\n",
    "\n",
    "## Comparison\n",
    "### Qualitative comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fmzU3W6zzsJ1"
   },
   "source": [
    "When zoomed in, we can see the clear difference between each resulting image, especially in edges with high contrast. 16x16 compression yields pixelated images that are clearer to see than 8x8 compression and larger in size. Furthermore, we can see clear difference between high compression and low compression in both block sizes. High compression yielded more 'noisy' image than low compression. In low compression, the difference between 8x8 compression and 16x16 is not visible by just perceiving the image. However, in high compression, the quality is visibly lower in 16x16 than 8x8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display reconstructed images\n",
    "f, axarr = plt.subplots(2,2, figsize = (48,48))\n",
    "axarr[0,0].imshow(reconstructed[0], cmap = \"gray\")\n",
    "axarr[0,0].set_title(\"8x8 low compression\", fontsize = 40)\n",
    "axarr[0,1].imshow(reconstructed[1], cmap = \"gray\")\n",
    "axarr[0,1].set_title(\"8x8 high compression\", fontsize = 40)\n",
    "axarr[1,0].imshow(reconstructed[2], cmap = \"gray\")\n",
    "axarr[1,0].set_title(\"16x16 low compression\", fontsize = 40)\n",
    "axarr[1,1].imshow(reconstructed[3], cmap = \"gray\")\n",
    "axarr[1,1].set_title(\"16x16 high compression\", fontsize = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZaPJeQkOpuz-"
   },
   "source": [
    "### Quantitative comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0X-ZJlN_zgeW"
   },
   "source": [
    "The table below shows quantitative analysis of the resulting reconstructed images. As expected, 8x8 compression results in lower root mean squared error than 16x16 compression and lower compression ratio. This is due to the fact that less data is lost in 8x8 compression. It retains more information. Also, we can notice that the number of floating point operations is significantly lower for 16x16 compression blocks. High compression yields greater compression ratio compared to low compression in both 8x8 and 16x16 blocks. It also yields higher RMSE. This is expected since more data is lost when higher compression type is used. It corresponds to a quantization table that has large values, which zeros out all high frequency components in the image. Moreover, # of flops in high compression is almost half of that in low compression (for both 8x8 and 16x16 blocks). This is due to the fact that run-length encoding takes significantly shorter time to decode highly compressed images compared to low compressed images; there are more zeros in highly compressed images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of compression achieved\n",
    "imarr = reshape_image(im)\n",
    "\n",
    "#Compute the number of bits of the np array image\n",
    "size_before = imarr.size * imarr.itemsize * 8 \n",
    "print(\"Size in bits of image before compression: \", size_before)\n",
    "\n",
    "# Number of bits in encoded is the length of encoded. Each element in the string corresponds to a bit in reality.\n",
    "\n",
    "size_after = []\n",
    "for i in range(len(encoded)):\n",
    "    size_after.append(len(encoded[i]))\n",
    "\n",
    "##################################################\n",
    "\n",
    "# Number of floating point operations\n",
    "\n",
    "#Dummy operation to measure the time for a single operation\n",
    "start = timeit.default_timer()\n",
    "x = 2837*3847\n",
    "stop = timeit.default_timer()\n",
    "single_FLOP = stop - start\n",
    "\n",
    "##################################################\n",
    "\n",
    "\n",
    "# Quality of compressed image (RMSE)\n",
    "\n",
    "rms_error =[]\n",
    "for i in range(len(reconstructed)):\n",
    "    if(i > 1):\n",
    "        #Then reconstructed image is based on 16x16 compression,\n",
    "        #Therefore, the image should be compared with the reshaped image having\n",
    "        #box_size 16\n",
    "        imarr = reshape_image(im,16)\n",
    "    rms_error.append(rmse(imarr, reconstructed[i]))\n",
    "\n",
    "\n",
    "# Print results\n",
    "comp_type = [\"8x8 low\", \"8x8 high\", \"16x16 low\", \"16x16 high\",]\n",
    "# intialise data of lists.\n",
    "data = {'compression type':comp_type,\n",
    "        'size in bits': size_after,\n",
    "        'compression ratio':[size_before/x for x in size_after] ,\n",
    "        \"# of flops\": [t/single_FLOP for t in execution_time],\n",
    "        'RMSE': rms_error}\n",
    " \n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    " \n",
    "# Print the output.\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NM33oMq16u7y"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<h1 align=\"center\"><font face = \"Times New Roman\">JPEG2000 compression</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4kkgGf87jF25"
   },
   "source": [
    "## Methodology\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## breakdown of the process\n",
    "JPEG2000 is yet another image compression algorithm under the category of transform compression algorithms. Similar to JPEG, JPEG2000 is also lossy and depends on giving less attention to the edges information than to illumination information. \n",
    "\n",
    "Similar to the analysis in the JPEG part, we repeatedly encode and decode the image according to the given decomposition levels and quantization table. The huffman encoded sequence is saved in *encoded* array to be later used in computing the compression ratio. All reconstructed images are saved in reconstructed array, and the execution time for each compression type is saved in *execution_time* array.\n",
    "\n",
    "To encode and decode, 2 main functions in main_2000.py were utilized; namely, encode() and decode(). Both functions arrange all encoder_2000.py and decoder_2000.py functions sequentially in the following way: \n",
    "\n",
    "![jpeg2000 pipeline diagram](images/JPEG2000_diagram.png)\n",
    "\n",
    "\n",
    "## Algorithm \n",
    "The algorithm is done through the following steps: \n",
    "\n",
    "1- downsample the image by 4\n",
    "\n",
    "2- pass the result through 1D filters once vertically and once horizontally for once in each direction\n",
    "\n",
    "3- the resulting 4 images should be LL, LH, HL, HH corresponding to the filters and directions they were applied on. So you should have something like this:\n",
    "\n",
    "![DWT results](images/npvvj.gif)\n",
    "\n",
    "4- quantize the each image. High frequencies should be quantized in less bits (less attention) and low frequencies should get more bits (less quantization). This means that the quantization should increase in this direction LL, LH, HL, HH.\n",
    "\n",
    "5- now read the images in zigzag fashion to get the zeros in the end\n",
    "\n",
    "6- similar to the JPEG compression you pass the serialized images through run length coding and entropy coding (huffman in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using this image to run the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image\n",
    "\n",
    "im = Image.open('images/contrast.jpg')\n",
    "im = im.convert(\"L\")\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantization tables and DWT decomposition levels\n",
    "\n",
    "# LL, LL\n",
    "levels = [\n",
    "             [0, #LL \n",
    "                 [[0]] #LL again\n",
    "             ]\n",
    "        ]\n",
    "\n",
    "## LL block will be quantized by 2s, LH by 2s, HL by 3 and HH by 4.\n",
    "quantization_Array = [1, 2, 3, 4]\n",
    "\n",
    "\n",
    "encoded = [] # An array containing all 4 huffcodes resulting from the encoding process\n",
    "reconstructed = [] # Array containing all 4 reconstructed images\n",
    "execution_time = [] # Execution time for each compression\n",
    "\n",
    "start = timeit.default_timer()\n",
    "# Apply the the decomposition stated in levels, and perform [2,2,3,4] quantization on it. \n",
    "\n",
    "huffcoded, code_dict, length, aspect_ratio = m2.encode(im, levels, quantization_Array)\n",
    "encoded.append(huffcoded)\n",
    "reconstructed.append( m2.decode( huffcoded, code_dict, length, quantization_Array, aspect_ratio) )\n",
    "execution_time.append(timeit.default_timer() - start)\n",
    "\n",
    "print(\"Execution time: \", execution_time[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_Array = [1, 64, 128, 256]\n",
    "\n",
    "start = timeit.default_timer() \n",
    "#LL, LL with quantization [1, 64, 128, 256]\n",
    "\n",
    "huffcoded, code_dict, length, aspect_ratio = m2.encode(im, levels, quantization_Array)\n",
    "encoded.append(huffcoded)\n",
    "reconstructed.append( m2.decode( huffcoded, code_dict, length, quantization_Array, aspect_ratio) )\n",
    "execution_time.append(timeit.default_timer() - start)\n",
    "\n",
    "print(\"Execution time: \", execution_time[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [[1,[[0]]]]\n",
    "\n",
    "## LL block will be quantized by 1s, LH by 2s, HL by 3 and HH by 4.\n",
    "quantization_Array = [1, 2, 3, 4]\n",
    "\n",
    "start = timeit.default_timer() \n",
    "# HH, LL with quantization [1, 2, 3, 4]\n",
    "\n",
    "huffcoded, code_dict, length, aspect_ratio = m2.encode(im, levels, quantization_Array)\n",
    "encoded.append(huffcoded)\n",
    "reconstructed.append( m2.decode( huffcoded, code_dict, length, quantization_Array, aspect_ratio) )\n",
    "execution_time.append(timeit.default_timer() - start)\n",
    "\n",
    "print(\"Execution time: \", execution_time[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_Array = [1, 64, 128, 256]\n",
    "\n",
    "start = timeit.default_timer() \n",
    "\n",
    "huffcoded, code_dict, length, aspect_ratio = m2.encode(im, levels, quantization_Array)\n",
    "encoded.append(huffcoded)\n",
    "reconstructed.append( m2.decode( huffcoded, code_dict, length, quantization_Array, aspect_ratio) )\n",
    "execution_time.append(timeit.default_timer() - start)\n",
    "\n",
    "print(\"Execution time: \", execution_time[3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zd3n6iEKREsP"
   },
   "source": [
    "\n",
    "\n",
    "## Comparison\n",
    "### Qualitative comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare between the output of 4 different runs of the algorithm alternating between 2 different decompositions and 2 different quantizations. Each image is titled with the corresponding run parameters. Most of the compression effects would be especially obvious in edges with high contrast. Higher values for compression yields pixelated images around edges. This is due to the fact that we quantize the information of high frequencies in lower number of bits which are responsible for crisp edges. \n",
    "\n",
    "To understand the titles of the images let's explain the title of the first image \"LL,LL with quantization [1,2,3,4]\" This means that when we decompose the image we end up with 4 different images of which we decompose the LL image again and give that LL image the least quantization, or in other words, giving the second level decomposition LL the highest number of bits. That being said, the last 2 images we did the opposite and since we gave more bits to high frequency image decompositions, we end up with crisp edges for the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2,2, figsize = (48,48))\n",
    "axarr[0,0].imshow(Image.fromarray(reconstructed[0]).resize((im.size[0],im.size[1])), cmap = \"gray\")\n",
    "axarr[0,0].set_title(\"LL,LL with quantization [1,2,3,4]\", fontsize = 40)\n",
    "axarr[0,1].imshow(Image.fromarray(reconstructed[1]).resize((im.size[0],im.size[1])), cmap = \"gray\")\n",
    "axarr[0,1].set_title(\"LL,LL with quantization [1, 64, 128, 256]\", fontsize = 40)\n",
    "axarr[1,0].imshow(Image.fromarray(reconstructed[2]).resize((im.size[0],im.size[1])), cmap = \"gray\")\n",
    "axarr[1,0].set_title(\"HH,LL with quantization [1,2,3,4]\", fontsize = 40)\n",
    "axarr[1,1].imshow(Image.fromarray(reconstructed[3]).resize((im.size[0],im.size[1])), cmap = \"gray\")\n",
    "axarr[1,1].set_title(\"HH,LL with quantization [1, 64, 128, 256]\", fontsize = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zd3n6iEKREsP"
   },
   "source": [
    "\n",
    "\n",
    "### Quantitative comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows quantitative analysis of the resulting reconstructed images. As expected, [LL,LL] with high quantization results in the highest compression ratio and root mean squared error. This is because most of information content of this particular image was included in the high frequencies which suffered severe quantization. This is why the edges ended up blurry. Also, we can notice that the number of floating point operations is highest for this case. \n",
    "\n",
    "\n",
    "On the other hand, [HH,LL] resulted in the lower compression because when we retained most of the image information that was included in the high frequencies and therefore less information was compressed since it was too little to begin with. high quantization resulted in more compression for that particular case as expected, yet however, it doesn't achieve optimal results because of the same reasons. \n",
    "\n",
    "To get a better understanding, we can compare case 1 and 3 or \"LL,LL with quantization [1,2,3,4]\" against \"HL,LL with quantization [1,2,3,4]\". For the naked eye, you might not be able tell the difference, however, case 1 achieves higher compression because it quantized the high frequency components more than the lower frequencies. \n",
    "\n",
    "\n",
    "From this discussion we can infere that most of the useful information is present in the low frequency components and in order to achieve the highest compression rates without losing much quality, we need to quantize the information in the high frequencies and retain the information in the low frequencies (illumination). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of compression achieved\n",
    "imarr,_ = check_image(im, 3)\n",
    "\n",
    "#Compute the number of bits of the np array image\n",
    "size_before = imarr.size * imarr.itemsize * 8 \n",
    "size_after = []\n",
    "for i in range(len(encoded)):\n",
    "    size_after.append(len(encoded[i]))\n",
    "\n",
    "\n",
    "##################################################\n",
    "\n",
    "\n",
    "# Quality of compressed image (RMSE)\n",
    "\n",
    "rms_error =[]\n",
    "for i in range(len(reconstructed)):\n",
    "    rms_error.append(rmse(imarr, reconstructed[i]))\n",
    "\n",
    "\n",
    "# Print results\n",
    "comp_type = [\"LL,LL [1,2,3,4]\", \"LL,LL [2, 64, 128, 256]\", \"HH,LL [1,2,3,4]\", \"HH,LL [2, 64, 128, 256]\",]\n",
    "# intialise data of lists.\n",
    "data = {'compression type':comp_type,\n",
    "        'size in bits': size_after,\n",
    "        'compression ratio':[size_before/x for x in size_after] ,\n",
    "        \"# of flops\": [t/single_FLOP for t in execution_time],\n",
    "        'RMSE': rms_error}\n",
    " \n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "# Print the output.\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YZD5n8d9Uayo"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<h2 align=\"center\"><font face = \"Times New Roman\"> Division of work amongst team members</font></h2>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SoJaWej4Fuq"
   },
   "source": [
    "Alaa Roshdy\n",
    "\n",
    "*   DCT\n",
    "*   main encoder and decoder\n",
    "*   Documentation for decoding functions\n",
    "*   Qualitative and quantitative comparison\n",
    "*   Debugging and testing\n",
    "\n",
    "Mohamed Adham Mahrous\n",
    "\n",
    "*   Huffman encoding and decoding\n",
    "*   Runlength encoding and decoding\n",
    "*   DWT filtering and dwt_serialization \n",
    "*   Documentation for encoding functions\n",
    "*   Qualitative and quantitative comparison\n",
    "*   Debugging and testing\n",
    "\n",
    "Mohamed Kasem Saber\n",
    "\n",
    "*   DCT and IDCT\n",
    "*   Serialize and deserialize\n",
    "*   Quantize and dequantize\n",
    "*   Documentation for the above\n",
    "*   Get sub_images and get_reconstructed_image\n",
    "\n",
    "Mohamed Mostafa Hamed\n",
    "\n",
    "*   DWT encoder and decoder\n",
    "*   DWT_serialize and DWT_deserialize\n",
    "*   Documentation for DWT\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
